{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device found.\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(\"MPS device found.\")\n",
    "else:\n",
    "    print(\"MPS device not found. Using CPU.\")\n",
    "    mps_device = torch.device(\"cpu\")\n",
    "\n",
    "# Example: Move a tensor to the MPS device\n",
    "tensor = torch.ones(1, device=mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1\n",
      "CUDA available: False\n",
      "No GPU available, PyTorch will use CPU\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch if not already installed\n",
    "#!pip install torch\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available, PyTorch will use CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists at the specified path.\n",
      "File exists at the specified path.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path.cwd() / \"data\" / \"Amazon.mat\"\n",
    "save_path = Path.cwd() / \"result\"\n",
    "print(f\"File {'exists' if os.path.exists(path) else 'does not exist'} at the specified path.\")\n",
    "\n",
    "print(f\"File {'exists' if os.path.exists(save_path) else 'does not exist'} at the specified path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device MPS is available using : cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(\"Using device:\", device)\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device MPS is available using :\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb82d978",
    "outputId": "2eee333f-5874-45f3-e643-298df9eb3e38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the .mat file:\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'Network', 'Label', 'Attributes'])\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import os\n",
    "import csv  # # updated here\n",
    "\n",
    "file_path = path # '/content/drive/MyDrive/pes_sem4_basepaper1/data/Amazon.mat'\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "  mat_data = scipy.io.loadmat(file_path)\n",
    "  print(\"Keys in the .mat file:\")\n",
    "  print(mat_data.keys())\n",
    "else:\n",
    "  print(f\"File not found at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yBxwAAp_a1W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYqLvdSA_awn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MPjxZonQ_foj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run of this cell completed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import scipy.io as sio\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "def load_dataset(args):\n",
    "    config = dict()\n",
    "    config['beta'] = 0\n",
    "    config['norm'] = True\n",
    "    config['clusters'] = args.clusters\n",
    "    if args.dataset == 'Amazon':\n",
    "        adj1, adj2, features, ano_label = load_mat(args.dataset)\n",
    "        features, _ = preprocess_features(features)\n",
    "        raw_features = features\n",
    "        #config['cutting'] = 25\n",
    "        config['cutting'] = min(getattr(args, 'cutting', 25), 25)  # updated here\n",
    "        config['lamb'] = 1\n",
    "        config['alpha'] = 0.8\n",
    "        config['norm'] = False\n",
    "    elif args.dataset == 'YelpChi':\n",
    "        adj1, adj2, features, ano_label = load_mat(args.dataset)\n",
    "        features, _ = preprocess_features(features)\n",
    "        raw_features = features\n",
    "        config['cutting'] = 3\n",
    "        config['lamb'] = 1\n",
    "        config['alpha'] = 0.8\n",
    "        config['norm'] = False\n",
    "    elif args.dataset == 'dblp':\n",
    "        adj1, adj2, features, ano_label = load_dblp_graph()\n",
    "        raw_features = features\n",
    "        config['cutting'] = 20\n",
    "        config['lamb'] = 0.01\n",
    "        config['alpha'] = 1\n",
    "    elif args.dataset == 'imdb':\n",
    "        adj1, adj2, features, ano_label = load_imdb_graph()\n",
    "        raw_features = features\n",
    "        config['cutting'] = 15\n",
    "        config['lamb'] = 0.01\n",
    "        config['alpha'] = 1\n",
    "    elif args.dataset == 'cert':\n",
    "        adj1, adj2, features, ano_label = load_cert_graph()\n",
    "        raw_features = features\n",
    "        config['cutting'] = 7\n",
    "        config['lamb'] = 0.1\n",
    "        config['alpha'] = 0.8\n",
    "    else:\n",
    "        adj1, adj2, features, ano_label = load_mat(args.dataset)\n",
    "        raw_features = features.todense()\n",
    "        features = raw_features\n",
    "        config['cutting'] = 7\n",
    "        config['beta'] = 1\n",
    "        if args.dataset == 'BlogCatalog':\n",
    "            config['clusters'] = 5\n",
    "            config['lamb'] = 0.01\n",
    "            config['alpha'] = 0\n",
    "    config['ft_size'] = features.shape[1]\n",
    "    raw_adj1 = (adj1 + sp.eye(adj1.shape[0])).todense()\n",
    "    raw_adj1 = torch.FloatTensor(raw_adj1[np.newaxis])\n",
    "    if adj2 is None:\n",
    "        raw_adj2 = None\n",
    "    else:\n",
    "        raw_adj2 = (adj2 + sp.eye(adj2.shape[0])).todense()\n",
    "        raw_adj2 = torch.FloatTensor(raw_adj2[np.newaxis])\n",
    "    raw_features = torch.FloatTensor(raw_features[np.newaxis])\n",
    "    features = torch.FloatTensor(features[np.newaxis])\n",
    "    return raw_features, features, adj1, adj2, ano_label, raw_adj1, raw_adj2, config\n",
    "\n",
    "def sparse_to_tuple(sparse_mx, insert_batch=False):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    \"\"\"Set insert_batch=True if you want to insert a batch dimension.\"\"\"\n",
    "\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        if insert_batch:\n",
    "            coords = np.vstack((np.zeros(mx.row.shape[0]), mx.row, mx.col)).transpose()\n",
    "            values = mx.data\n",
    "            shape = (1,) + mx.shape\n",
    "        else:\n",
    "            coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "            values = mx.data\n",
    "            shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "\n",
    "def RandomDropEdge(x, adj_t, drop_percent=0.05):\n",
    "    percent = drop_percent / 2\n",
    "    row_idx, col_idx = x.nonzero().T\n",
    "\n",
    "    index_list = []\n",
    "    for i in range(len(row_idx)):\n",
    "        index_list.append((row_idx[i], col_idx[i]))\n",
    "\n",
    "    edge_num = int(len(row_idx) / 2)  # 9228 / 2\n",
    "    add_drop_num = int(edge_num * percent / 2)\n",
    "    aug_adj = copy.deepcopy(adj_t)\n",
    "\n",
    "    edge_idx = [i for i in range(edge_num)]\n",
    "    drop_idx = random.sample(edge_idx, add_drop_num)\n",
    "\n",
    "    for i in drop_idx:\n",
    "        aug_adj[index_list[i][0]][index_list[i][1]] = 0\n",
    "        aug_adj[index_list[i][1]][index_list[i][0]] = 0\n",
    "\n",
    "    node_num = x.shape[0]\n",
    "    l = [(i, j) for i in range(node_num) for j in range(i)]\n",
    "    add_list = random.sample(l, add_drop_num)\n",
    "\n",
    "    for i in add_list:\n",
    "        aug_adj[i[0]][i[1]] = 1\n",
    "        aug_adj[i[1]][i[0]] = 1\n",
    "\n",
    "    adj = aug_adj.to_sparse()\n",
    "    return adj\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features.todense(), sparse_to_tuple(features)\n",
    "\n",
    "\n",
    "def normalize_adj_tensor(raw_adj, dataset='BlogCatalog'):\n",
    "    # if dataset =='BlogCatalog':\n",
    "    #     return raw_adj\n",
    "    adj = raw_adj[0, :, :]\n",
    "    row_sum = torch.sum(adj, 0)\n",
    "    r_inv = torch.pow(row_sum, -0.5).flatten()\n",
    "    r_inv[torch.isinf(r_inv)] = 0.\n",
    "    adj = torch.mm(adj, torch.diag_embed(r_inv))\n",
    "    adj = torch.mm(torch.diag_embed(r_inv), adj)\n",
    "    adj = adj.unsqueeze(0)\n",
    "    return adj\n",
    "\n",
    "\n",
    "def normalize_score(ano_score):\n",
    "    ano_score = ((ano_score - np.min(ano_score)) / (\n",
    "            np.max(ano_score) - np.min(ano_score)))\n",
    "    return ano_score\n",
    "\n",
    "\n",
    "def process_dis(init_value, cutting_dis_array):\n",
    "    r_inv = np.power(init_value, -0.5).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    cutting_dis_array = cutting_dis_array.dot(sp.diags(r_inv))\n",
    "    cutting_dis_array = sp.diags(r_inv).dot(cutting_dis_array)\n",
    "    return cutting_dis_array\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "\n",
    "def load_mat(dataset):\n",
    "    \"\"\"Load .mat dataset.\"\"\"\n",
    "\n",
    "    #file_path = f\"/content/drive/MyDrive/pes_sem4_basepaper1/data/{dataset}.mat\"\n",
    "    file_path = Path.cwd() / \"data\" / f\"{dataset}.mat\"\n",
    "\n",
    "    \n",
    "    #data = sio.loadmat(\"./data/{}.mat\".format(dataset))\n",
    "    data = sio.loadmat(file_path)\n",
    "    label = data['Label'] if ('Label' in data) else data['gnd']\n",
    "    attr = data['Attributes'] if ('Attributes' in data) else data['X']\n",
    "    network = data['Network'] if ('Network' in data) else data['A']\n",
    "    adj = sp.csr_matrix(network)\n",
    "    feat = sp.lil_matrix(attr)\n",
    "\n",
    "    ano_labels = np.squeeze(np.array(label))\n",
    "    adj1 = adj\n",
    "    adj2 = None\n",
    "    return adj1, adj2, feat, ano_labels\n",
    "\n",
    "\n",
    "from scipy import sparse\n",
    "def load_dblp_graph():\n",
    "    if not os.path.exists('./data/DBLP_anomaly.mat'):\n",
    "        data = sio.loadmat('./data/DBLP')\n",
    "        features = data['feature'].astype(np.float64)\n",
    "        label = np.zeros((features.shape[0], 1))\n",
    "        p = 0.07\n",
    "        index = np.random.choice(features.shape[0], int(p * features.shape[0]), replace=False)\n",
    "        label[index] = 1\n",
    "        feature_anomaly_idx = index[:index.shape[0]//2]\n",
    "        edge_anomaly_idx = index[index.shape[0]//2:]\n",
    "        features[feature_anomaly_idx] += np.random.normal(2, 10, size=(feature_anomaly_idx.shape[0], features.shape[1]))\n",
    "        adj1 = data['net_APA'] + np.random.binomial(1, 0.5, size=(edge_anomaly_idx.shape[0], data['net_APA'].shape[1]))\n",
    "        adj2 = data['net_APCPA'] + np.random.binomial(1, 0.5, size=(edge_anomaly_idx.shape[0], data['net_APCPA'].shape[1]))\n",
    "        data = {'feature': features, 'label': label, 'adj1': adj1, 'adj2': adj2}\n",
    "        sio.savemat('./data/dblp_anomaly.mat', data)\n",
    "    else:\n",
    "        data = sio.loadmat('./data/dblp_anomaly.mat')\n",
    "        features = data['feature']\n",
    "        adj1 = data['adj1']\n",
    "        adj2 = data['adj2']\n",
    "        label = data['label'].reshape(-1, )\n",
    "    # print('size of anomalies:', sum(label))\n",
    "    # features = sparse.csr_matrix(features)\n",
    "    adj1 = sparse.csr_matrix(adj1)\n",
    "    adj2 = sparse.csr_matrix(adj2)\n",
    "    return adj1, adj2, features, label\n",
    "\n",
    "\n",
    "def load_imdb_graph():\n",
    "    if not os.path.exists('./data/imdb5k_anomaly.mat'):\n",
    "        data = sio.loadmat('./data/imdb5k.mat')\n",
    "        features = data['feature'].astype(np.float64)\n",
    "        label = np.zeros((features.shape[0], 1))\n",
    "        p = 0.07\n",
    "        index = np.random.choice(features.shape[0], int(p * features.shape[0]), replace=False)\n",
    "        label[index] = 1\n",
    "        feature_anomaly_idx = index[:index.shape[0]//2]\n",
    "        edge_anomaly_idx = index[index.shape[0]//2:]\n",
    "        features[feature_anomaly_idx] += np.random.normal(2, 10, size=(feature_anomaly_idx.shape[0], features.shape[1]))\n",
    "        adj1 = data['MAM'] + np.random.binomial(1, 0.5, size=(edge_anomaly_idx.shape[0], data['MAM'].shape[1]))\n",
    "        adj2 = data['MDM'] + np.random.binomial(1, 0.5, size=(edge_anomaly_idx.shape[0], data['MDM'].shape[1]))\n",
    "        data = {'feature': features, 'label': label, 'adj1': adj1, 'adj2': adj2}\n",
    "        sio.savemat('./data/imdb5k_anomaly.mat', data)\n",
    "    else:\n",
    "        data = sio.loadmat('./data/imdb5k_anomaly.mat')\n",
    "        features = data['feature']\n",
    "        adj1 = data['adj1']\n",
    "        adj2 = data['adj2']\n",
    "        label = data['label'].reshape(-1, )\n",
    "    # print('size of anomalies:', sum(label))\n",
    "    # features = sparse.csr_matrix(features)\n",
    "    adj1 = sparse.csr_matrix(adj1)\n",
    "    adj2 = sparse.csr_matrix(adj2)\n",
    "    return adj1, adj2, features, label\n",
    "\n",
    "\n",
    "def load_cert_graph(d=100):\n",
    "    v2 = pkl.load(open('./CERT/logon.pkl', 'rb'))\n",
    "    v1 = pkl.load(open('./CERT/email.pkl', 'rb'))\n",
    "    malicious_user = pkl.load(open('./CERT/label.pkl', 'rb'))['label']\n",
    "    label = []\n",
    "    email_pc_dict = v2['pc_dict']\n",
    "    email_user_dict = v2['user_dict']\n",
    "    overlapped_idx = []\n",
    "    for item, key in email_pc_dict.items():\n",
    "        if item in v1['pc_dict']:\n",
    "            overlapped_idx.append(v1['pc_dict'][item])\n",
    "    v2['graph'] = v2['graph'][overlapped_idx, :]\n",
    "    v2['weight'] = v2['weight'][overlapped_idx, :]\n",
    "    overlapped_idx = []\n",
    "    for item, key in email_user_dict.items():\n",
    "        if item in v1['user_dict']:\n",
    "            overlapped_idx.append(v1['user_dict'][item])\n",
    "        if item in malicious_user:\n",
    "            label.append(1)\n",
    "        else:\n",
    "            label.append(0)\n",
    "    v2['graph'] = v2['graph'][:, overlapped_idx]\n",
    "    v2['weight'] = v2['weight'][:, overlapped_idx]\n",
    "    if not os.path.exists('./CERT/logon_edge_list.txt'):\n",
    "        n_nodes = v2['weight'].shape[0]\n",
    "        with open('./CERT/logon_edge_list.txt', 'w') as f:\n",
    "            for i in range(len(v2['graph'])):\n",
    "                idx = np.nonzero(v2['graph'][i, :])[0]\n",
    "                for j in idx:\n",
    "                    f.write('{} {}\\n'.format(i, j + n_nodes))\n",
    "    if not os.path.exists('./CERT/email_edge_list.txt'):\n",
    "        n_nodes = v1['weight'].shape[0]\n",
    "        with open('./CERT/email_edge_list.txt', 'w') as f:\n",
    "            for i in range(len(v1['graph'])):\n",
    "                idx = np.nonzero(v1['graph'][i, :])[0]\n",
    "                for j in idx:\n",
    "                    f.write('{} {}\\n'.format(i, j + n_nodes))\n",
    "    if not os.path.exists('./CERT/email_edge_list_emb_{}'.format(d)):\n",
    "        os.system(\"python ./deepwalk/main.py --representation-size {} --input ./CERT/email_edge_list.txt\"\n",
    "                  \" --output ./CERT/email_edge_list_emb_{}\".format(d, d))\n",
    "    if not os.path.exists('./CERT/logon_edge_list_emb_{}'.format(d)):\n",
    "        os.system(\"python ./deepwalk/main.py --representation-size {} --input ./CERT/logon_edge_list.txt\"\n",
    "                  \" --output ./CERT/logon_edge_list_emb_{}\".format(d, d))\n",
    "    v1_feature = np.zeros((v1['weight'].shape[0], d))\n",
    "    v2_feature = np.zeros((v2['weight'].shape[0], d))\n",
    "    with open('./CERT/logon_edge_list_emb_{}'.format(d), 'r') as f:\n",
    "        next(f)\n",
    "        for line in f.readlines():\n",
    "            line = list(map(float, line.split()))\n",
    "            if line[0] >= 1000:\n",
    "                v1_feature[int(line[0]) - 1000] = line[1:]\n",
    "    with open('./CERT/email_edge_list_emb_{}'.format(d), 'r') as f:\n",
    "        next(f)\n",
    "        for line in f.readlines():\n",
    "            line = list(map(float, line.split()))\n",
    "            if line[0] >= 1000:\n",
    "                v2_feature[int(line[0]) - 1000] = line[1:]\n",
    "    v1_feats = v1_feature\n",
    "    v2_feats = v2_feature\n",
    "    # v1_adj = np.dot(v1['graph'], v1['graph'].transpose())\n",
    "    # v2_adj = np.dot(v2['graph'], v2['graph'].transpose())\n",
    "    v1_adj = v1['graph']\n",
    "    v2_adj = v2['graph']\n",
    "    label = np.array(label).reshape(-1, 1)\n",
    "    adj1 = sparse.csr_matrix(v1_adj)\n",
    "    adj2 = sparse.csr_matrix(v2_adj)\n",
    "    return adj1, adj2, v2_feats, label\n",
    "\n",
    "'''\n",
    "# compute the distance between each node\n",
    "def calc_distance(adj, seq):\n",
    "    dis_array = torch.zeros((adj.shape[0], adj.shape[1]))\n",
    "    row = adj.shape[0]\n",
    "    for i in range(row):\n",
    "        # print(i)\n",
    "        node_index = torch.argwhere(adj[i, :] > 0)\n",
    "        for j in node_index:\n",
    "            dis = torch.sqrt(torch.sum((seq[i] - seq[j]) * (seq[i] - seq[j])))\n",
    "            dis_array[i][j] = dis\n",
    "    return dis_array\n",
    "'''\n",
    "\n",
    "\n",
    "def calc_distance(adj, seq):\n",
    "    # adj: (N,N) torch tensor; seq: (N,D)\n",
    "    idx = (adj > 0).nonzero(as_tuple=False)  # updated here\n",
    "    if idx.numel() == 0:  # updated here\n",
    "        return torch.zeros_like(adj, device=seq.device)  # updated here\n",
    "    src = idx[:, 0]  # updated here\n",
    "    dst = idx[:, 1]  # updated here\n",
    "    diff = seq[src] - seq[dst]  # updated here\n",
    "    dis = torch.sqrt((diff * diff).sum(1))  # updated here\n",
    "    dis_array = torch.zeros_like(adj, device=seq.device)  # updated here\n",
    "    dis_array[src, dst] = dis  # updated here\n",
    "    return dis_array  # updated here\n",
    "\n",
    "\n",
    "def get_cos_similar(v1: list, v2: list):\n",
    "    num = float(np.dot(v1, v2))  # 向量点乘\n",
    "    denom = np.linalg.norm(v1) * np.linalg.norm(v2)  # 求模长的乘积\n",
    "    return 0.5 + 0.5 * (num / denom) if denom != 0 else 0\n",
    "\n",
    "\n",
    "def calc_sim(adj_matrix, attr_matrix):\n",
    "    row = adj_matrix.shape[0]\n",
    "    col = adj_matrix.shape[1]\n",
    "    dis_array = np.zeros((row, col))\n",
    "    for i in range(row):\n",
    "        # print(i)\n",
    "        node_index = np.argwhere(adj_matrix[i, :] > 0)[:, 0]\n",
    "        for j in node_index:\n",
    "            dis = get_cos_similar(attr_matrix[i].tolist(), attr_matrix[j].tolist())\n",
    "            dis_array[i][j] = dis\n",
    "\n",
    "    return dis_array\n",
    "\n",
    "\"\"\"\n",
    "def graph_nsgt(dis_array, adj):\n",
    "    #dis_array = dis_array.cuda()\n",
    "    dis_array = dis_array.to(adj.device)  # updated here (was: dis_array = dis_array.cuda())\n",
    "    row = dis_array.shape[0]\n",
    "    dis_array_u = dis_array * adj\n",
    "    mean_dis = dis_array_u[dis_array_u != 0].mean()\n",
    "    for i in range(row):\n",
    "        node_index = torch.argwhere(adj[i, :] > 0)\n",
    "        if node_index.shape[0] != 0:\n",
    "            max_dis = dis_array[i, node_index].max()\n",
    "            min_dis = mean_dis\n",
    "            if max_dis > min_dis:\n",
    "                random_value = (max_dis - min_dis) * np.random.random_sample() + min_dis\n",
    "                cutting_edge = torch.argwhere(dis_array[i, node_index[:, 0]] > random_value)\n",
    "                if cutting_edge.shape[0] != 0:\n",
    "                    adj[i, node_index[cutting_edge[:, 0]]] = 0\n",
    "    adj = adj + adj.T\n",
    "    adj[adj > 1] = 1\n",
    "    return adj\n",
    "\"\"\"\n",
    "def graph_nsgt(dis_array, adj):\n",
    "    dis_array = dis_array.to(adj.device)  # updated here (was: dis_array = dis_array.cuda())\n",
    "    row = dis_array.shape[0]\n",
    "    dis_array_u = dis_array * adj\n",
    "    nonzero = dis_array_u[dis_array_u != 0]  # updated here (guard for empty case)\n",
    "    mean_dis = nonzero.mean() if nonzero.numel() > 0 else torch.tensor(0.0, device=adj.device)  # updated here\n",
    "    for i in range(row):\n",
    "        node_index = torch.argwhere(adj[i, :] > 0)\n",
    "        if node_index.shape[0] != 0:\n",
    "            max_dis = dis_array[i, node_index].max()\n",
    "            min_dis = mean_dis\n",
    "            if max_dis > min_dis:\n",
    "                random_value = (max_dis - min_dis) * np.random.random_sample() + float(min_dis.item())  # updated here\n",
    "                cutting_edge = torch.argwhere(dis_array[i, node_index[:, 0]] > random_value)\n",
    "                if cutting_edge.shape[0] != 0:\n",
    "                    adj[i, node_index[cutting_edge[:, 0]]] = 0\n",
    "    adj = adj + adj.T\n",
    "    adj[adj > 1] = 1\n",
    "    return adj\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['figure.figsize'] = (8.5, 7.5)\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def draw_pdf(message, ano_label, dataset):\n",
    "    with PdfPages('{}-TAM.pdf'.format(dataset)) as pdf:\n",
    "        normal_message_all = message[ano_label == 0]\n",
    "        abnormal_message_all = message[ano_label == 1]\n",
    "        message_all = [normal_message_all, abnormal_message_all]\n",
    "        mu_0 = np.mean(message_all[0])\n",
    "        sigma_0 = np.std(message_all[0])\n",
    "        print('The mean of normal {}'.format(mu_0))\n",
    "        print('The std of normal {}'.format(sigma_0))\n",
    "        mu_1 = np.mean(message_all[1])\n",
    "        sigma_1 = np.std(message_all[1])\n",
    "        print('The mean of abnormal {}'.format(mu_1))\n",
    "        print('The std of abnormal {}'.format(sigma_1))\n",
    "        n, bins, patches = plt.hist(message_all, bins=30, normed=1, label=['Normal', 'Abnormal'])\n",
    "        y_0 = mlab.normpdf(bins, mu_0, sigma_0)\n",
    "        y_1 = mlab.normpdf(bins, mu_1, sigma_1)\n",
    "        plt.plot(bins, y_0, color='steelblue', linestyle='--', linewidth=7.5)\n",
    "        plt.plot(bins, y_1, color='darkorange', linestyle='--', linewidth=7.5)\n",
    "        plt.yticks(fontsize=30)\n",
    "        plt.xticks(fontsize=30)\n",
    "        plt.legend(loc='upper left', fontsize=30)\n",
    "        plt.title(''.format(dataset), fontsize=25)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def draw_pdf_str_attr(message, ano_label, str_ano_label, attr_ano_label, dataset):\n",
    "    with PdfPages('{}-TAM.pdf'.format(dataset)) as pdf:\n",
    "        normal_message_all = message[ano_label == 0]\n",
    "        str_abnormal_message_all = message[str_ano_label == 1]\n",
    "        attr_abnormal_message_all = message[attr_ano_label == 1]\n",
    "        message_all = [normal_message_all, str_abnormal_message_all, attr_abnormal_message_all]\n",
    "\n",
    "        mu_0 = np.mean(message_all[0])\n",
    "        sigma_0 = np.std(message_all[0])\n",
    "        print('The mean of normal {}'.format(mu_0))\n",
    "        print('The std of normal {}'.format(sigma_0))\n",
    "        mu_1 = np.mean(message_all[1])\n",
    "        sigma_1 = np.std(message_all[1])\n",
    "        print('The mean of str_abnormal {}'.format(mu_1))\n",
    "        print('The std of str_abnormal {}'.format(sigma_1))\n",
    "        mu_2 = np.mean(message_all[2])\n",
    "        sigma_2 = np.std(message_all[2])\n",
    "        print('The mean of attt_abnormal {}'.format(mu_2))\n",
    "        print('The std of attt_abnormal {}'.format(sigma_2))\n",
    "        n, bins, patches = plt.hist(message_all, bins=30, normed=1, label=['Normal', 'Structural Abnormal', 'Contextual Abnormal'])\n",
    "        y_0 = mlab.normpdf(bins, mu_0, sigma_0)\n",
    "        y_1 = mlab.normpdf(bins, mu_1, sigma_1)\n",
    "        y_2= mlab.normpdf(bins, mu_2, sigma_2)  #\n",
    "\n",
    "        plt.plot(bins, y_0, color='steelblue', linestyle='--', linewidth=3.5)\n",
    "        plt.plot(bins, y_1, color='darkorange', linestyle='--', linewidth=3.5)\n",
    "        plt.plot(bins, y_2, color='green', linestyle='--', linewidth=3.5)\n",
    "\n",
    "        plt.xlabel('TAM-based Affinity', fontsize=25)\n",
    "        plt.ylabel('Number of Samples', size=25)\n",
    "        plt.yticks(fontsize=25)\n",
    "        plt.xticks(fontsize=25)\n",
    "        plt.legend(loc='upper left', fontsize=18)\n",
    "        plt.title('{}'.format(dataset), fontsize=25)\n",
    "        plt.show()\n",
    "print('run of this cell completed');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fX1E2ExruUrB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run of this cell completed\n"
     ]
    }
   ],
   "source": [
    "################################################################## model.py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from utils import *\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "\n",
    "class DiffPoolingBlock(torch.nn.Module):\n",
    "    def __init__(self, in_dim, n_clusters, tau=2, dim=-1, n_layer=1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.ModuleList()\n",
    "        self.linear.append(nn.Linear(in_dim, n_clusters))\n",
    "        for i in range(n_layer-1):\n",
    "            self.linear.append(nn.Linear(n_clusters, n_clusters))\n",
    "        self.softmax = nn.Softmax(dim=dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.tau = tau\n",
    "        self.dim = dim\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.linear:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        out = h\n",
    "        for i in range(len(self.linear)):\n",
    "            out = self.activation(torch.mm(adj, self.linear[i](out)) + 1e-15)\n",
    "        return self.softmax(out/self.tau)\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_ft, out_ft, act, bias=True):\n",
    "        super(GCN, self).__init__()\n",
    "        self.fc = nn.Linear(in_ft, out_ft, bias=False)\n",
    "        self.act = nn.PReLU() if act == 'prelu' else act\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_ft))\n",
    "            self.bias.data.fill_(0.0)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, seq, adj, sparse=False):\n",
    "        seq_fts = self.fc(seq)\n",
    "        if sparse:\n",
    "            out = torch.unsqueeze(torch.spmm(adj, torch.squeeze(seq, 0)), 0)\n",
    "        else:\n",
    "            out = torch.mm(adj, seq_fts)\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        return self.act(out)\n",
    "\n",
    "\n",
    "class AvgReadout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AvgReadout, self).__init__()\n",
    "\n",
    "    def forward(self, seq):\n",
    "        return torch.mean(seq, 1)\n",
    "\n",
    "\n",
    "class MaxReadout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxReadout, self).__init__()\n",
    "\n",
    "    def forward(self, seq):\n",
    "        return torch.max(seq, 1).values\n",
    "\n",
    "\n",
    "class MinReadout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MinReadout, self).__init__()\n",
    "\n",
    "    def forward(self, seq):\n",
    "        return torch.min(seq, 1).values\n",
    "\n",
    "\n",
    "class WSReadout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WSReadout, self).__init__()\n",
    "\n",
    "    def forward(self, seq, query):\n",
    "        query = query.permute(0, 2, 1)\n",
    "        sim = torch.matmul(seq, query)\n",
    "        sim = F.softmax(sim, dim=1)\n",
    "        sim = sim.repeat(1, 1, 64)\n",
    "        out = torch.mul(seq, sim)\n",
    "        out = torch.sum(out, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, n_h, activation, readout, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.read_mode = readout\n",
    "        self.gcn1 = GCN(n_in, 2 * n_h, activation)\n",
    "        self.gcn2 = GCN(2 * n_h, n_h, activation)\n",
    "        self.act = nn.PReLU()\n",
    "        self.fc1 = nn.Linear(n_h, 2 * n_h, bias=False)\n",
    "        self.diffpool = DiffPoolingBlock(n_in, n_clusters=config['clusters'], dim=-1)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        if readout == 'max':\n",
    "            self.read = MaxReadout()\n",
    "        elif readout == 'min':\n",
    "            self.read = MinReadout()\n",
    "        elif readout == 'avg':\n",
    "            self.read = AvgReadout()\n",
    "        elif readout == 'weighted_sum':\n",
    "            self.read = WSReadout()\n",
    "        self.tau = 1\n",
    "        self.lamb = config['lamb']\n",
    "        self.alpha = config['alpha']\n",
    "        self.beta = config['beta']\n",
    "        self.norm = config['norm']\n",
    "\n",
    "    def normalize_adj_tensor(self, adj):\n",
    "        row_sum = torch.sum(adj, 0)\n",
    "        r_inv = torch.pow(row_sum, -0.5).flatten()\n",
    "        r_inv = torch.nan_to_num(r_inv, posinf=0, neginf=0)\n",
    "        adj = torch.mm(adj, torch.diag_embed(r_inv))\n",
    "        adj = torch.mm(torch.diag_embed(r_inv), adj)\n",
    "        return adj\n",
    "\n",
    "    def forward(self, seq, adj, raw_adj, adj2=None,  raw_adj2=None, sparse=False):\n",
    "        if adj2 is not None:\n",
    "            feat1 = self.gcn2(self.gcn1(seq, adj[0]), adj[0])\n",
    "            feat2 = self.gcn2(self.gcn1(seq, adj2[0]), adj2[0])\n",
    "            feat = (feat1 + feat2)/2\n",
    "            s_adj = (adj[0] + adj2[0])/2\n",
    "        else:\n",
    "            feat = self.gcn2(self.gcn1(seq, adj[0]), adj[0])\n",
    "            s_adj = adj[0]\n",
    "            raw_adj2 = raw_adj\n",
    "        assign = self.diffpool(seq, s_adj)\n",
    "        self.G = assign\n",
    "        raw_adj = (raw_adj[0] + raw_adj2[0])/2\n",
    "        con_loss = self.graph_contrastive_loss(feat, feat, assign, raw_adj)\n",
    "        cluster_sim = self.alpha * torch.mm(assign, assign.T) + raw_adj * (1 - self.alpha)\n",
    "        if self.norm:\n",
    "            cluster_sim = self.normalize_adj_tensor(cluster_sim)\n",
    "        if adj2 is None:\n",
    "            view_consistency = 0\n",
    "        else:\n",
    "            view_consistency = torch.norm(feat1 - feat2, dim=1, p=2).mean()\n",
    "        loss, message_sum1 = self.max_message(feat, cluster_sim)\n",
    "        fc1 = self.fc1(feat)\n",
    "        if self.beta != 0:\n",
    "            loss += self.beta * self.reg_edge(fc1, adj[0])\n",
    "        return feat, cluster_sim, loss + self.lamb * (con_loss + view_consistency)\n",
    "\n",
    "    def graph_contrastive_loss(self, v1, v2, assignment, adj, test_phase=False):\n",
    "        if v1.shape[0] > 10000 and not test_phase:\n",
    "            idx = torch.randperm(v1.shape[0])[:8000]\n",
    "        else:\n",
    "            idx = torch.arange(v1.shape[0])\n",
    "        v1, v2, assignment = v1[idx], v2[idx], assignment[idx]\n",
    "        adj = adj[idx, :][:, idx]\n",
    "        v = (v1 + v2)/2\n",
    "        sim = self.cosine_sim(v, v, self.tau)\n",
    "        O = self.alpha * torch.matmul(assignment, assignment.T) + (1 - self.alpha) * adj\n",
    "        n = assignment.shape[0]\n",
    "        loss = torch.norm(O / O.sum(dim=1).clamp_min(0.01) - sim, 2, dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "    def view_consistency(self, seq, adj, adj2, sparse=False):\n",
    "        if adj2 is None:\n",
    "            return 0\n",
    "        feat1 = self.gcn2(self.gcn1(seq, adj[0]), adj[0])\n",
    "        feat2 = self.gcn2(self.gcn1(seq, adj2[0]), adj2[0])\n",
    "        view_consistency = torch.norm(feat1 - feat2, dim=1, p=2)\n",
    "        return view_consistency * self.lamb\n",
    "\n",
    "    def cosine_sim(self, x1, x2, eps=1e-15, temperature=1):\n",
    "        w1 = x1.norm(p=2, dim=1, keepdim=True)\n",
    "        w2 = x2.norm(p=2, dim=1, keepdim=True)\n",
    "        return torch.matmul(x1, x2.t()) / ((w1 * w2.t()).clamp(min=eps) * temperature)\n",
    "\n",
    "    def reg_edge(self, emb, adj):\n",
    "        emb = emb / torch.norm(emb, dim=-1, keepdim=True)\n",
    "        sim_u_u = torch.mm(emb, emb.T)\n",
    "        adj_inverse = (1 - adj)\n",
    "        sim_u_u = sim_u_u * adj_inverse\n",
    "        sim_u_u_no_diag = torch.sum(sim_u_u, 1)\n",
    "        row_sum = torch.sum(adj_inverse, 1)\n",
    "        r_inv = torch.pow(row_sum, -1)\n",
    "        r_inv[torch.isinf(r_inv)] = 0.\n",
    "        sim_u_u_no_diag = sim_u_u_no_diag * r_inv\n",
    "        loss_reg = torch.sum(sim_u_u_no_diag)\n",
    "        return loss_reg\n",
    "\n",
    "    def max_message(self, feature, adj_matrix):\n",
    "        feature = feature / torch.norm(feature, dim=-1, keepdim=True)\n",
    "        sim_matrix = torch.mm(feature, feature.T)\n",
    "        sim_matrix = torch.squeeze(sim_matrix) * adj_matrix\n",
    "        sim_matrix[torch.isinf(sim_matrix)] = 0\n",
    "        sim_matrix[torch.isnan(sim_matrix)] = 0\n",
    "        row_sum = torch.sum(adj_matrix, 0)\n",
    "        r_inv = torch.pow(row_sum, -1).flatten()\n",
    "        r_inv[torch.isinf(r_inv)] = 0.\n",
    "        message = torch.sum(sim_matrix, 1)\n",
    "        message = message * r_inv\n",
    "        return - torch.sum(message), message\n",
    "\n",
    "    def inference(self, feature, adj_matrix):\n",
    "        feature = feature / torch.norm(feature, dim=-1, keepdim=True)\n",
    "        sim_matrix = torch.mm(feature, feature.T)\n",
    "        sim_matrix = torch.squeeze(sim_matrix) * adj_matrix\n",
    "        row_sum = torch.sum(adj_matrix, 0)\n",
    "        r_inv = torch.pow(row_sum, -1).flatten()\n",
    "        r_inv[torch.isinf(r_inv)] = 0.\n",
    "        message = torch.sum(sim_matrix, 1)\n",
    "        message = message * r_inv\n",
    "        return message\n",
    "\n",
    "    def evaluation(self, score, ano_label):\n",
    "        auc = roc_auc_score(ano_label, score)\n",
    "        AP = average_precision_score(ano_label, score, average='macro', pos_label=1, sample_weight=None)\n",
    "        print('AP::{:.4f}, AUC:{:.4f}'.format(AP, auc))\n",
    "print('run of this cell completed');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1Ay8bgV62uk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Oq-0f7N__6qx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run of this cell started\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################################################################################################################################VERSION @\n",
    "print('run of this cell started');\n",
    "# ============================== main.py (CPU-only) ==============================\n",
    "import os\n",
    "import os.path\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#from utils import *          # provides: load_dataset, graph_nsgt, normalize_adj_tensor, calc_distance, normalize_score\n",
    "#from model import Model      # model class\n",
    "\n",
    "# ----- Force CPU -----\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "device = device #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# Where to cache distances / outputs\n",
    "save_path = save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "VAEltIbb_6nA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run of this cell completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main(args):\n",
    "    # Repro\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    # Load and preprocess data\n",
    "    raw_features, features, adj1, adj2, ano_label, raw_adj1, raw_adj2, config = load_dataset(args)\n",
    "\n",
    "    # Build models & optimizers (use config['cutting'] everywhere)\n",
    "    optimiser_list, model_list = [], []\n",
    "    for _ in range(config['cutting']):\n",
    "        model = Model(config['ft_size'], args.embedding_dim, 'prelu', args.readout, config).to(device)\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        optimiser_list.append(optimiser)\n",
    "        model_list.append(model)\n",
    "\n",
    "    start = time.time()\n",
    "    total_epoch = args.num_epoch * config['cutting']\n",
    "\n",
    "    # Initial cut-adj (view 1)\n",
    "    new_adj_list1 = [raw_adj1]\n",
    "    all_cut_adj1 = torch.cat(new_adj_list1)\n",
    "\n",
    "    # Distance cache (view 1)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    distance1_file = os.path.join(save_path, f\"{args.dataset}_distance1.npy\")\n",
    "    if os.path.exists(distance1_file):\n",
    "        dis_array1 = torch.tensor(np.load(distance1_file), dtype=torch.float32)\n",
    "    else:\n",
    "        dis_array1 = calc_distance(raw_adj1[0, :, :], raw_features[0, :, :])\n",
    "        np.save(distance1_file, dis_array1.detach().cpu().numpy())\n",
    "\n",
    "    # Optional second view\n",
    "    if raw_adj2 is not None:\n",
    "        new_adj_list2 = [raw_adj2]\n",
    "        all_cut_adj2 = torch.cat(new_adj_list2)\n",
    "\n",
    "        dist2_path = os.path.join(save_path, f\"{args.dataset}_distance2.npy\")\n",
    "        if os.path.exists(dist2_path):\n",
    "            dis_array2 = torch.tensor(np.load(dist2_path), dtype=torch.float32)\n",
    "        else:\n",
    "            # IMPORTANT: use raw_adj2 for view-2 distance\n",
    "            dis_array2 = calc_distance(raw_adj2[0, :, :], raw_features[0, :, :])\n",
    "            np.save(dist2_path, dis_array2.detach().cpu().numpy())\n",
    "    else:\n",
    "        all_cut_adj2 = None\n",
    "        dis_array2 = None\n",
    "\n",
    "    index = 0\n",
    "    message_mean_list = []\n",
    "\n",
    "    # Per-cut training\n",
    "    for n_cut in range(config['cutting']):\n",
    "        message_list = []\n",
    "        optimiser_list[index].zero_grad()\n",
    "        model_list[index].train()\n",
    "\n",
    "        # Cut adj for view 1\n",
    "        cut_adj1 = graph_nsgt(dis_array1, all_cut_adj1[0, :, :])\n",
    "        cut_adj1 = cut_adj1.unsqueeze(0)\n",
    "        adj_norm1 = normalize_adj_tensor(cut_adj1, args.dataset)\n",
    "\n",
    "        # Cut adj for view 2\n",
    "        if all_cut_adj2 is not None:\n",
    "            cut_adj2 = graph_nsgt(dis_array2, all_cut_adj2[0, :, :])\n",
    "            cut_adj2 = cut_adj2.unsqueeze(0)\n",
    "            adj_norm2 = normalize_adj_tensor(cut_adj2, args.dataset)\n",
    "        else:\n",
    "            adj_norm2 = None\n",
    "\n",
    "        # Epoch loop (clear grads each step)\n",
    "        for epoch in range(args.num_epoch):\n",
    "            optimiser_list[index].zero_grad()\n",
    "            node_emb, cluster_sim, loss = model_list[index].forward(\n",
    "                features[0], adj_norm1, raw_adj1, adj_norm2, raw_adj2\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimiser_list[index].step()\n",
    "\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{n_cut * args.num_epoch + epoch + 1}/{total_epoch}], \"\n",
    "                    f\"time: {time.time() - start:.4f}, Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # Scoring\n",
    "        message_sum = (\n",
    "            model_list[index].inference(node_emb, cluster_sim)\n",
    "            + model_list[index].view_consistency(features[0], adj_norm1, adj_norm2)\n",
    "        )\n",
    "        message_list.append(torch.unsqueeze(message_sum.detach().cpu(), 0))\n",
    "\n",
    "        # Update bases for next cut\n",
    "        all_cut_adj1[0, :, :] = torch.squeeze(cut_adj1)\n",
    "        if all_cut_adj2 is not None:\n",
    "            all_cut_adj2[0, :, :] = torch.squeeze(cut_adj2)\n",
    "\n",
    "        index += 1\n",
    "\n",
    "        # Aggregate & evaluate\n",
    "        message_list = torch.mean(torch.cat(message_list), 0)\n",
    "        message_mean_list.append(torch.unsqueeze(message_list, 0))\n",
    "        message_mean_cut = torch.mean(torch.cat(message_mean_list), 0)\n",
    "        message_mean = np.array(message_mean_cut.detach())\n",
    "        score = 1 - normalize_score(message_mean)\n",
    "        model_list[index - 1].evaluation(score, ano_label)\n",
    "\n",
    "    print(\"Total time (s):\", time.time() - start)\n",
    "print('run of this cell completed');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMskJwpo_6jt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_42AJH7_6f8",
    "outputId": "96e5ced4-ad79-496b-87b1-c642ebb38aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Amazon\n",
      "Epoch [100/12500], time: 197.3624, Loss: -9071.0615\n",
      "Epoch [200/12500], time: 395.7949, Loss: -9473.6924\n",
      "Epoch [300/12500], time: 590.5679, Loss: -9718.1484\n",
      "Epoch [400/12500], time: 786.6324, Loss: -9866.6289\n",
      "Epoch [500/12500], time: 985.6993, Loss: -9959.4053\n",
      "AP::0.2151, AUC:0.5731\n",
      "Epoch [600/12500], time: 1188.4782, Loss: -8502.3740\n",
      "Epoch [700/12500], time: 1389.1498, Loss: -9178.1084\n",
      "Epoch [800/12500], time: 1587.3178, Loss: -9573.1719\n",
      "Epoch [900/12500], time: 1772.4200, Loss: -9795.6006\n",
      "Epoch [1000/12500], time: 1953.9954, Loss: -9919.8291\n",
      "AP::0.3033, AUC:0.6359\n",
      "Epoch [1100/12500], time: 2164.6496, Loss: -8376.7090\n",
      "Epoch [1200/12500], time: 2363.5132, Loss: -9047.5752\n",
      "Epoch [1300/12500], time: 2561.8472, Loss: -9472.9902\n",
      "Epoch [1400/12500], time: 2759.0827, Loss: -9722.7236\n",
      "Epoch [1500/12500], time: 2943.2562, Loss: -9870.9375\n",
      "AP::0.3667, AUC:0.6941\n",
      "Epoch [1600/12500], time: 3131.8009, Loss: -8112.7080\n",
      "Epoch [1700/12500], time: 3329.5097, Loss: -8837.6719\n",
      "Epoch [1800/12500], time: 3514.6346, Loss: -9307.3701\n",
      "Epoch [1900/12500], time: 3696.5628, Loss: -9596.7607\n",
      "Epoch [2000/12500], time: 3887.7302, Loss: -9776.4346\n",
      "AP::0.4175, AUC:0.7503\n",
      "Epoch [2100/12500], time: 4076.6620, Loss: -7879.5317\n",
      "Epoch [2200/12500], time: 4254.0170, Loss: -8709.4941\n",
      "Epoch [2300/12500], time: 4431.2915, Loss: -9253.5693\n",
      "Epoch [2400/12500], time: 4614.5398, Loss: -9585.3525\n",
      "Epoch [2500/12500], time: 4793.0381, Loss: -9785.7803\n",
      "AP::0.4301, AUC:0.7762\n",
      "Epoch [2600/12500], time: 4976.8199, Loss: -8626.8555\n",
      "Epoch [2700/12500], time: 5161.9483, Loss: -9208.7354\n",
      "Epoch [2800/12500], time: 5349.3282, Loss: -9561.1641\n",
      "Epoch [2900/12500], time: 5543.4026, Loss: -9773.4961\n",
      "Epoch [3000/12500], time: 5736.4898, Loss: -9902.5439\n",
      "AP::0.4525, AUC:0.7997\n",
      "Epoch [3100/12500], time: 5927.7945, Loss: -8120.2715\n",
      "Epoch [3200/12500], time: 6111.9259, Loss: -8838.7559\n",
      "Epoch [3300/12500], time: 6301.3479, Loss: -9300.5352\n",
      "Epoch [3400/12500], time: 6480.4663, Loss: -9586.2266\n",
      "Epoch [3500/12500], time: 6659.9368, Loss: -9767.8408\n",
      "AP::0.4743, AUC:0.8179\n",
      "Epoch [3600/12500], time: 6839.3786, Loss: -8024.5957\n",
      "Epoch [3700/12500], time: 7113.4658, Loss: -8737.6895\n",
      "Epoch [3800/12500], time: 7336.1247, Loss: -9217.2607\n",
      "Epoch [3900/12500], time: 7538.7979, Loss: -9520.6992\n",
      "Epoch [4000/12500], time: 7728.6901, Loss: -9717.7725\n",
      "AP::0.5546, AUC:0.8424\n",
      "Epoch [4100/12500], time: 7919.0251, Loss: -7809.5771\n",
      "Epoch [4200/12500], time: 8119.9140, Loss: -8596.9785\n",
      "Epoch [4300/12500], time: 8315.4678, Loss: -9122.3770\n",
      "Epoch [4400/12500], time: 8512.3625, Loss: -9461.4395\n",
      "Epoch [4500/12500], time: 8710.3371, Loss: -9681.3252\n",
      "AP::0.5799, AUC:0.8488\n",
      "Epoch [4600/12500], time: 8908.7263, Loss: -7390.1548\n",
      "Epoch [4700/12500], time: 9115.4276, Loss: -8328.9619\n",
      "Epoch [4800/12500], time: 9310.4602, Loss: -8971.9561\n",
      "Epoch [4900/12500], time: 9501.4444, Loss: -9368.8594\n",
      "Epoch [5000/12500], time: 9696.9213, Loss: -9615.4209\n",
      "AP::0.5808, AUC:0.8524\n",
      "Epoch [5100/12500], time: 9896.3294, Loss: -7972.2197\n",
      "Epoch [5200/12500], time: 10096.4976, Loss: -8800.0146\n",
      "Epoch [5300/12500], time: 10297.0191, Loss: -9311.1943\n",
      "Epoch [5400/12500], time: 10499.8438, Loss: -9609.7725\n",
      "Epoch [5500/12500], time: 10691.1230, Loss: -9787.4658\n",
      "AP::0.5824, AUC:0.8544\n",
      "Epoch [5600/12500], time: 10892.3976, Loss: -7706.8105\n",
      "Epoch [5700/12500], time: 11088.3072, Loss: -8547.1543\n",
      "Epoch [5800/12500], time: 11283.7260, Loss: -9115.2734\n",
      "Epoch [5900/12500], time: 11455.1668, Loss: -9471.6914\n",
      "Epoch [6000/12500], time: 11633.1640, Loss: -9693.9170\n",
      "AP::0.5978, AUC:0.8588\n",
      "Epoch [6100/12500], time: 11817.1408, Loss: -6771.1113\n",
      "Epoch [6200/12500], time: 12001.7104, Loss: -7819.4287\n",
      "Epoch [6300/12500], time: 12190.0499, Loss: -8604.7627\n",
      "Epoch [6400/12500], time: 12373.9426, Loss: -9120.1387\n",
      "Epoch [6500/12500], time: 12565.9903, Loss: -9445.1904\n",
      "AP::0.6066, AUC:0.8623\n",
      "Epoch [6600/12500], time: 12761.6388, Loss: -7914.0854\n",
      "Epoch [6700/12500], time: 12946.5495, Loss: -8752.6201\n",
      "Epoch [6800/12500], time: 13131.2397, Loss: -9272.9805\n",
      "Epoch [6900/12500], time: 13328.7833, Loss: -9577.3027\n",
      "Epoch [7000/12500], time: 13523.2318, Loss: -9759.9346\n",
      "AP::0.6085, AUC:0.8637\n",
      "Epoch [7100/12500], time: 13723.2369, Loss: -7540.2798\n",
      "Epoch [7200/12500], time: 13923.5673, Loss: -8445.0469\n",
      "Epoch [7300/12500], time: 14117.4562, Loss: -9051.9756\n",
      "Epoch [7400/12500], time: 14310.0669, Loss: -9422.3906\n",
      "Epoch [7500/12500], time: 14505.5179, Loss: -9649.0273\n",
      "AP::0.6157, AUC:0.8677\n",
      "Epoch [7600/12500], time: 14704.2521, Loss: -7708.1743\n",
      "Epoch [7700/12500], time: 14903.2871, Loss: -8589.4961\n",
      "Epoch [7800/12500], time: 15098.9751, Loss: -9171.4434\n",
      "Epoch [7900/12500], time: 15295.1429, Loss: -9524.7236\n",
      "Epoch [8000/12500], time: 15495.5498, Loss: -9731.9795\n",
      "AP::0.6210, AUC:0.8701\n",
      "Epoch [8100/12500], time: 15693.1678, Loss: -7866.2759\n",
      "Epoch [8200/12500], time: 15889.8664, Loss: -8701.4482\n",
      "Epoch [8300/12500], time: 16088.1750, Loss: -9232.3145\n",
      "Epoch [8400/12500], time: 16285.4551, Loss: -9554.6963\n",
      "Epoch [8500/12500], time: 16477.3657, Loss: -9749.9336\n",
      "AP::0.6219, AUC:0.8722\n",
      "Epoch [8600/12500], time: 16671.5012, Loss: -7747.6963\n",
      "Epoch [8700/12500], time: 16859.8474, Loss: -8569.4209\n",
      "Epoch [8800/12500], time: 17053.3401, Loss: -9108.5723\n",
      "Epoch [8900/12500], time: 17248.1882, Loss: -9453.7256\n",
      "Epoch [9000/12500], time: 17444.3704, Loss: -9671.9287\n",
      "AP::0.6244, AUC:0.8727\n",
      "Epoch [9100/12500], time: 17643.4902, Loss: -7281.0059\n",
      "Epoch [9200/12500], time: 17838.9695, Loss: -8215.3623\n",
      "Epoch [9300/12500], time: 18038.6663, Loss: -8872.6143\n",
      "Epoch [9400/12500], time: 18237.4606, Loss: -9298.6982\n",
      "Epoch [9500/12500], time: 18435.5087, Loss: -9568.2969\n",
      "AP::0.6185, AUC:0.8724\n",
      "Epoch [9600/12500], time: 18635.3927, Loss: -7859.6460\n",
      "Epoch [9700/12500], time: 18834.6305, Loss: -8675.7490\n",
      "Epoch [9800/12500], time: 19030.1771, Loss: -9229.2812\n",
      "Epoch [9900/12500], time: 19229.4240, Loss: -9563.8105\n",
      "Epoch [10000/12500], time: 19428.4482, Loss: -9762.5361\n",
      "AP::0.6246, AUC:0.8742\n",
      "Epoch [10100/12500], time: 19622.5570, Loss: -7835.8823\n",
      "Epoch [10200/12500], time: 19820.8660, Loss: -8628.3389\n",
      "Epoch [10300/12500], time: 20017.6350, Loss: -9162.5010\n",
      "Epoch [10400/12500], time: 20200.5006, Loss: -9498.8926\n",
      "Epoch [10500/12500], time: 20406.8819, Loss: -9710.1973\n",
      "AP::0.6297, AUC:0.8759\n",
      "Epoch [10600/12500], time: 20613.0281, Loss: -8567.4707\n",
      "Epoch [10700/12500], time: 20810.5056, Loss: -9111.3389\n",
      "Epoch [10800/12500], time: 20998.6558, Loss: -9465.1670\n",
      "Epoch [10900/12500], time: 21189.2227, Loss: -9692.6367\n",
      "Epoch [11000/12500], time: 21384.2297, Loss: -9839.4102\n",
      "AP::0.6314, AUC:0.8770\n",
      "Epoch [11100/12500], time: 21574.5947, Loss: -7488.3315\n",
      "Epoch [11200/12500], time: 21771.5133, Loss: -8454.0938\n",
      "Epoch [11300/12500], time: 21974.9436, Loss: -9087.7500\n",
      "Epoch [11400/12500], time: 22192.0157, Loss: -9467.3789\n",
      "Epoch [11500/12500], time: 22393.5255, Loss: -9695.3242\n",
      "AP::0.6341, AUC:0.8787\n",
      "Epoch [11600/12500], time: 22590.4203, Loss: -7614.7197\n",
      "Epoch [11700/12500], time: 22786.8511, Loss: -8508.3574\n",
      "Epoch [11800/12500], time: 23046.8893, Loss: -9113.9717\n",
      "Epoch [11900/12500], time: 23239.4072, Loss: -9480.3428\n",
      "Epoch [12000/12500], time: 23420.1638, Loss: -9698.3633\n",
      "AP::0.6331, AUC:0.8784\n",
      "Epoch [12100/12500], time: 23603.2070, Loss: -7194.8237\n",
      "Epoch [12200/12500], time: 23786.1299, Loss: -8196.3730\n",
      "Epoch [12300/12500], time: 23979.8145, Loss: -8885.4805\n",
      "Epoch [12400/12500], time: 24184.4480, Loss: -9316.8545\n",
      "Epoch [12500/12500], time: 24392.0278, Loss: -9584.1797\n",
      "AP::0.6366, AUC:0.8795\n",
      "Total time (s): 24392.257376194\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Cluster-Aware Graph Anomaly Detection (CARE-demo)\")\n",
    "    parser.add_argument('--dataset', type=str, default='Amazon', help='Amazon | BlogCatalog | imdb | dblp')\n",
    "    parser.add_argument('--lr', type=float, default=1e-5)\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.0)\n",
    "    parser.add_argument('--seed', type=int, default=2)\n",
    "    parser.add_argument('--embedding_dim', type=int, default=128)\n",
    "    parser.add_argument('--num_epoch', type=int, default=500)\n",
    "    parser.add_argument('--readout', type=str, default='avg')      # max | min | avg | weighted_sum\n",
    "    parser.add_argument('--cutting', type=int, default=25)         # kept for CLI parity; config['cutting'] is used internally\n",
    "    parser.add_argument('--lamb', type=float, default=0.1)\n",
    "    parser.add_argument('--alpha', type=float, default=0.8)\n",
    "    parser.add_argument('--clusters', type=int, default=10)\n",
    "\n",
    "    # Friendly defaults for notebooks / scripts (no CLI args needed)\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    print('Dataset:', args.dataset)\n",
    "    main(args)\n",
    "# ===============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3nlVQyo_6cZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwGJW4iP_6ZN"
   },
   "outputs": [],
   "source": [
    "############\n",
    "'''\n",
    "Using device: cpu\n",
    "Dataset: Amazon\n",
    "Epoch [100/12500], time: 2040.4267, Loss: -9071.0615\n",
    "Epoch [200/12500], time: 3990.6654, Loss: -9473.6924\n",
    "Epoch [300/12500], time: 5898.8861, Loss: -9718.1484\n",
    "Epoch [400/12500], time: 7811.2736, Loss: -9866.6289\n",
    "Epoch [500/12500], time: 9725.5304, Loss: -9959.4072\n",
    "AP::0.2151, AUC:0.5731\n",
    "Epoch [600/12500], time: 11772.2434, Loss: -8502.3740\n",
    "Epoch [700/12500], time: 13748.7978, Loss: -9178.1084\n",
    "Epoch [800/12500], time: 15705.0023, Loss: -9573.1719\n",
    "Epoch [900/12500], time: 17665.5378, Loss: -9795.6006\n",
    "Epoch [1000/12500], time: 19604.1226, Loss: -9919.8291\n",
    "AP::0.3033, AUC:0.6359\n",
    "Epoch [1100/12500], time: 21619.7950, Loss: -8376.7080\n",
    "Epoch [1200/12500], time: 23552.9021, Loss: -9047.5762\n",
    "Epoch [1300/12500], time: 25474.5063, Loss: -9472.9902\n",
    "Epoch [1400/12500], time: 27392.5535, Loss: -9722.7246\n",
    "Epoch [1500/12500], time: 29350.0607, Loss: -9870.9375\n",
    "AP::0.3667, AUC:0.6941\n",
    "Epoch [1600/12500], time: 31415.8522, Loss: -8112.7080\n",
    "Epoch [1700/12500], time: 33385.9079, Loss: -8837.6709\n",
    "Epoch [1800/12500], time: 35374.1808, Loss: -9307.3701\n",
    "Epoch [1900/12500], time: 37346.4518, Loss: -9596.7598\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ajvhu1mQ_6VU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6pcLY82_6Rn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SpgsDXn_6Nm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7_g-8G4_6KT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
